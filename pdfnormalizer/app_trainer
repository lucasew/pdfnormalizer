#!/usr/bin/env nix-shell
#! nix-shell -i python -p python3Packages.tensorflow python3Packages.keras python3Packages.pandas python3Packages.scikit-learn
# vim:ft=python

from argparse import ArgumentParser
from pathlib import Path
import sqlite3

import numpy as np
import pandas as pd
import tensorflow as tf
from matplotlib import pyplot as plt
from pdfnormalizer.model import SubdivisionAction
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix
from sklearn.metrics import classification_report

parser = ArgumentParser()
parser.add_argument("-d", type=Path, help="db file to input", required=True)
parser.add_argument("-e", type=int, help="epochs", default=200)
parser.add_argument("-m", type=bool, help="mostrar matriz de confusão", default=False)
parser.add_argument("-a", type=bool, help="mostrar loss e acurácia", default=False)
parser.add_argument("-o", type=Path, help="onde salvar o modelo")
args = parser.parse_args()

conn = sqlite3.connect(str(args.d))
df = pd.read_sql("select * from datapoints", con=conn)
print(df)

df['classification_idx'] = list(map(
    lambda v: SubdivisionAction[v.replace("SubdivisionAction.", "")].value - 1,
    df['classification']
))
labels = list(map(
    lambda v: SubdivisionAction[v.replace("SubdivisionAction.", "")].name,
    list(df['classification'].unique())
))

print("labels", labels)
df['norm_depth'] = df['depth'] / 20

NUM_CLASSES = len(labels)
print("classes", NUM_CLASSES)


def extract_datapoints(df):
    return df[['norm_depth', 'x', 'y', 'sx', 'sy']], df[['classification_idx']]


df_train = df.sample(frac=0.5, random_state=0)
df_test = df.drop(df_train.index)

x_train, y_train = extract_datapoints(df_train)
y_train = np.array(tf.one_hot(y_train, NUM_CLASSES, axis=1)).reshape((-1, NUM_CLASSES))
# print(np.array(tf.one_hot(y_train, NUM_CLASSES, axis=1)).reshape((-1, NUM_CLASSES)))
x_test, y_test = extract_datapoints(df_test)
y_test = np.array(tf.one_hot(y_test, NUM_CLASSES, axis=1)).reshape((-1, NUM_CLASSES))
# print(oh_classification)

print(df)

model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(5, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(16, activation='relu'),
    # classificação multiclasse
    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')
])

class sigmoid_focal_crossentropy_loss():
    def __init__(
        self,
        alpha = 0.25,
        gamma = 0.,
        from_logits: bool = False,
    ):
        if gamma and gamma < 0:
            raise ValueError("Value of gamma should be greater than or equal to zero.")
        self.gamma = tf.constant(gamma)
        self.alpha = tf.constant(alpha)
        self.from_logits = from_logits

    def get_config(self):
        return dict(alpha=float(self.alpha), gamma=float(self.gamma), from_logits=self.from_logits)

    def __call__(self, y_true, y_pred):
        from tensorflow.keras import backend as K
        y_true = tf.one_hot(y_true, NUM_CLASSES)[0]
        # y_pred = tf.cast(y_pred)
        # y_true = tf.cast(y_true, dtype=y_pred.dtype)

        # Get the cross_entropy for each entry
        ce = K.binary_crossentropy(y_true, y_pred, from_logits=self.from_logits)

        # If logits are provided then convert the predictions into probabilities
        if self.from_logits:
            pred_prob = tf.sigmoid(y_pred)
        else:
            pred_prob = y_pred

        p_t = (y_true * pred_prob) + ((1 - y_true) * (1 - pred_prob))
        # alpha_factor = 1.0
        # modulating_factor = 1.0

        # if alpha:
        alpha = tf.cast(self.alpha, dtype=y_true.dtype)
        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)

        # if gamma:
        gamma = tf.cast(self.gamma, dtype=y_true.dtype)
        modulating_factor = tf.pow((1.0 - p_t), gamma)

        # compute the final loss and return
        return tf.reduce_sum(alpha_factor * modulating_factor * ce, axis=-1)


model.compile(
    optimizer='adam',
    # optimizer='sgd',
    # loss=sigmoid_focal_crossentropy_loss(gamma=2., ),
    loss='categorical_crossentropy',
    metrics=['accuracy', tf.keras.metrics.categorical_accuracy]
)

history = model.fit(x_train, y_train, epochs=args.e, batch_size=1024)
print(model.evaluate(x_test, y_test))

if args.o is not None:
    model.save(str(args.o))

if args.a:
    plt.plot(history.epoch, history.history['accuracy'], 'r', label="Acurácia")
    plt.plot(history.epoch, history.history['loss'], 'g', label="Loss")
    plt.legend()
    plt.show()

if args.m:
    predicted = model.predict(x_test)
    y_test = np.array(y_test)
    norm_predicted = np.cast['int8'](tf.argmax(predicted.T))
    print(y_test.dtype, y_test.shape, predicted.dtype, predicted.shape, norm_predicted.shape, norm_predicted.dtype)

    print(y_test, predicted)
    print(classification_report(tf.argmax(y_test, axis=1), tf.argmax(predicted, axis=1), labels=list(range(NUM_CLASSES)), target_names=labels))

    cm = confusion_matrix(tf.argmax(y_test, axis=1), tf.argmax(predicted, axis=1), labels=list(range(NUM_CLASSES)))
    print(cm)
    ConfusionMatrixDisplay(
        cm,
        display_labels=labels
    ).plot()

    plt.show()
